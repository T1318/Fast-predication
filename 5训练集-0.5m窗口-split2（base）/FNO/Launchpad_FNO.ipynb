{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import time\n",
    "import launchpad as lp\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.PDE_Net import DeepONet_NS, FNO2d, weight_init\n",
    "from utils.DataGenerate_FNO import Dataset_FNO, Normalize, InNormalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from argparse import Namespace\n",
    "from utils.utilities3 import *\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 0.5\n",
    "n_x = int(8/dx)\n",
    "n_y = int(3/dx)\n",
    "p_x = int(800/n_x+1)\n",
    "p_y = int(300/n_y+1)\n",
    "self_split = 2\n",
    "\n",
    "config = Namespace(\n",
    "\tproject_name = 'FNO',\n",
    "\tepochs = 100,\n",
    "\t\n",
    "\tdx = dx,\n",
    "\tn_x = n_x,\n",
    "\tn_y = n_y,\n",
    "\tp_x = p_x,\n",
    "\tp_y = p_y,\n",
    "\tself_split = self_split,\n",
    "\n",
    "\tpath_trained_model = r'trained_model',\n",
    "\tpath_distance = r'../train_data/distance',\n",
    "\tpath_label = r'../train_data/label',\n",
    "\tpath_distance_test = r'../test_data/distance',\n",
    "\tpath_label_test = r'../test_data/label',\n",
    "\n",
    "\tbatch_size = 15,\n",
    "\tmodes = 12,\n",
    "\twidth = 40,\n",
    "\n",
    "\tactivation = 'Sigmoid',\n",
    "# activation = torch.nn.__dict__[wandb.config.activation]()\n",
    "# optimizer = torch.optim.__dict__[wandb.config.optim_type](params=model.parameters(), lr=wandb.config.learning_rate)\n",
    "\toptim_type = 'SGD',\n",
    "\tlearning_rate = 0.0026392500276311696,\n",
    "\tdropout = 0,\n",
    "\tweight_decay = 0.00035181732255007376,\n",
    "\tmax_norm = 3.637374450623043,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "\t'method': 'random',\n",
    "}\n",
    "metric = {\n",
    "\t'name': 'loss_train',\n",
    "\t'goal': 'minimize'\n",
    "}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_config['parameters'] = {}\n",
    "sweep_config['parameters'].update({\n",
    "\t'project_name': {'value': 'FNO'},\n",
    "\t'epochs': {'value': 100},\n",
    "\t'dx': {'value': 0.5},\n",
    "\t'n_x': {'value': 16},\n",
    "\t'n_y': {'value': 6},\n",
    "\t'p_x': {'value': 51},\n",
    "\t'p_y': {'value': 51},\n",
    "\t'self_split': {'value': 2},\n",
    "\n",
    "\t'path_trained_model': {'value': r'trained_model'},\n",
    "\t'path_distance': {'value': r'../train_data/distance'},\n",
    "\t'path_label': {'value': r'../train_data/label'},\n",
    "\t'path_distance_test': {'value': r'../test_data/distance'},\n",
    "\t'path_label_test': {'value': r'../test_data/label'},\n",
    "})\n",
    "sweep_config['parameters'].update({\n",
    "\t'batch_size': {'values': [100,150]},\n",
    "\t'modes': {'distribution': 'q_uniform', 'q': 4, 'min': 8, 'max': 20},\n",
    "\t'width': {'distribution': 'q_uniform', 'q': 20, 'min': 20, 'max': 120},\n",
    "\n",
    "\t'activation': {'values': ['Tanh', 'ReLU', 'Sigmoid', 'LeakyReLU', 'GELU']},\n",
    "# activation = torch.nn.__dict__[wandb.config.activation]()\n",
    "# optimizer = torch.optim.__dict__[wandb.config.optim_type](params=model.parameters(), lr=wandb.config.learning_rate)\n",
    "\t'optim_type': {'values': ['Adam','SGD','AdamW']},\n",
    "\t'learning_rate': {'distribution': 'log_uniform_values', 'min': 1e-5, 'max': 1e-1},\n",
    "\t'dropout': {'distribution': 'q_uniform', 'q': 0.2, 'min': 0, 'max': 0.6},\n",
    "\t'weight_decay': {'distribution': 'log_uniform_values', 'min': 1e-5, 'max': 1e-1},\n",
    "\t'max_norm': {'distribution': 'uniform', 'min': 1, 'max': 10},\n",
    "})\n",
    "sweep_config['early_terminate'] = {\n",
    "\t'type':'hyperband',\n",
    "\t'min_iter':3,\n",
    "\t'eta':2,\n",
    "\t's':3\n",
    "}\n",
    "\n",
    "# 初始化sweep controller\n",
    "sweep_id = wandb.sweep(sweep_config, project='FNO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config):\n",
    "\ttrain_distance = torch.empty(0, config.p_x, config.p_y, 3)\n",
    "\ttrain_label = torch.empty(0, config.p_x, config.p_y, 1)\n",
    "\ttest_distance = torch.empty(0, config.p_x, config.p_y, 3)\n",
    "\ttest_label = torch.empty(0, config.p_x, config.p_y, 1)\n",
    "\n",
    "\tfile_list = os.listdir(config.path_distance)\n",
    "\tfile_list.sort()\n",
    "\tfor file in file_list:\n",
    "\t\tif file.endswith('.npy'):\n",
    "\t\t\t\tdistance = np.load(os.path.join(config.path_distance, file))\n",
    "\t\t\t\ttrain_distance = torch.cat((train_distance, torch.Tensor(distance)), 0)\n",
    "\n",
    "\tfile_list = os.listdir(config.path_label)\n",
    "\tfile_list.sort()\n",
    "\tfor file in file_list:\n",
    "\t\tif file.endswith('.npy'):\n",
    "\t\t\t\tlabel = np.load(os.path.join(config.path_label, file))\n",
    "\t\t\t\ttrain_label = torch.cat((train_label, torch.Tensor(label)), 0)\n",
    "\n",
    "\tfile_list = os.listdir(config.path_distance_test)\n",
    "\tfile_list.sort()\n",
    "\tfor file in file_list:\n",
    "\t\tif file.endswith('.npy'):\n",
    "\t\t\t\tdistance = np.load(os.path.join(config.path_distance_test, file))\n",
    "\t\t\t\ttest_distance = torch.cat((test_distance, torch.Tensor(distance)), 0)\n",
    "\n",
    "\tfile_list = os.listdir(config.path_label_test)\n",
    "\tfile_list.sort()\n",
    "\tfor file in file_list:\n",
    "\t\tif file.endswith('.npy'):\n",
    "\t\t\t\tlabel = np.load(os.path.join(config.path_label_test, file))\n",
    "\t\t\t\ttest_label = torch.cat((test_label, torch.Tensor(label)), 0)\n",
    "\n",
    "\ttrain_distance = torch.Tensor(train_distance)\n",
    "\ttrain_label = torch.Tensor(train_label)\n",
    "\ttest_distance = torch.Tensor(test_distance)\n",
    "\ttest_label = torch.Tensor(test_label)\n",
    "\n",
    "\tx_normalizer = GaussianNormalizer(train_distance)\n",
    "\tx_data = x_normalizer.encode(train_distance)\n",
    "\ty_normalizer = GaussianNormalizer(train_label)\n",
    "\ty_data = y_normalizer.encode(train_label)\n",
    "\n",
    "\tx_normalizer_test = GaussianNormalizer(test_distance)\n",
    "\tx_test = x_normalizer_test.encode(test_distance)\n",
    "\ty_normalizer_test = GaussianNormalizer(test_label)\n",
    "\ty_test = y_normalizer_test.encode(test_label)\n",
    "\n",
    "\ttrain_loader = DataLoader(torch.utils.data.TensorDataset(x_data, y_data), batch_size=config.batch_size, shuffle=True)\n",
    "\ttest_loader = DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "\treturn train_loader, test_loader, x_normalizer, y_normalizer, x_normalizer_test, y_normalizer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer,y_normalizer,device):\n",
    "\tmodel.train()\n",
    "\ttrain_loss_epoch = 0\n",
    "\tfor batch in train_loader:\n",
    "\t\tloss_train = 0\n",
    "\t\tdata_x,data_y = batch\n",
    "\n",
    "\t\tx = data_x.float().to(device)\n",
    "\t\ty = data_y.float().to(device)\n",
    "\t\t# batch_min = data_min.float().to(device)\n",
    "\t\t# batch_max = data_max.float().to(device)\n",
    "\n",
    "\t\tpred = model(x)\n",
    "\n",
    "\t\tpred = y_normalizer.decode(pred)\n",
    "\t\ty = y_normalizer.decode(y)\n",
    "\n",
    "\t\tloss_train = myloss(pred.clone(), y.clone())\n",
    "\t\tregularization_loss = 0\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tregularization_loss += torch.norm(param, p=2)\n",
    "\t\tloss_train = loss_train + config.weight_decay * regularization_loss\n",
    "\t\ttrain_loss_epoch = train_loss_epoch + loss_train.item()\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss_train.backward(retain_graph=True)\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), config.max_norm)\n",
    "\t\toptimizer.step()\n",
    "\ttrain_loss_epoch = train_loss_epoch / len(train_loader)\n",
    "\t# print('epoch:', epoch, 'loss_train:', train_loss_epoch)\n",
    "\t# scheduler.step(train_loss_epoch)\n",
    "\treturn model, optimizer, train_loss_epoch\n",
    "\n",
    "def eval_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer_test,y_normalizer_test,device):\n",
    "\tmodel.eval()\n",
    "\ttest_loss_epoch = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch_test in test_loader:\n",
    "\t\t\tloss_test = 0\n",
    "\t\t\ttest_x,test_y = batch_test\n",
    "\t\t\t\n",
    "\t\t\ttest_x = test_x.float().to(device)\t\t\t# [length,time_step,51,51,3]\n",
    "\t\t\ttest_y = test_y.float().to(device)\t\t\t# [length,time_step,51,51,3]\n",
    "\t\t\t# batch_min_test = test_min.float().to(device)\n",
    "\t\t\t# batch_max_test = test_max.float().to(device)\n",
    "\n",
    "\t\t\tpred_test = model(test_x)\n",
    "\n",
    "\t\t\tpred_test = y_normalizer_test.decode(pred_test)\n",
    "\t\t\ttest_y = y_normalizer_test.decode(test_y)\n",
    "\n",
    "\t\t\tloss_test = myloss(pred_test.clone(), test_y.clone())\n",
    "\n",
    "\t\t\ttest_loss_epoch = test_loss_epoch + loss_test.item()\n",
    "\t\ttest_loss_epoch = test_loss_epoch/len(test_loader)\n",
    "\t\t# print('epoch:', epoch, 'loss_test:', test_loss_epoch)\n",
    "\treturn test_loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(is_model_saved=False):\n",
    "\tnowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\t# 服务器里搜索时用的是: wandb.init(): 没有参数\n",
    "\twith wandb.init():\n",
    "\t\ttrain_loader, test_loader, x_normalizer, y_normalizer, x_normalizer_test, y_normalizer_test = create_dataloader(wandb.config)\n",
    "\t\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\t\tactivation = torch.nn.__dict__[wandb.config.activation]()\n",
    "\t\tmodel = FNO2d(wandb.config.modes, wandb.config.modes, wandb.config.width, activation, wandb.config.dropout)\n",
    "\t\tmodel.apply(weight_init)\n",
    "\t\tbegin_epoch = 0\n",
    "\n",
    "\t\tif is_model_saved:\n",
    "\t\t\tfile_list = os.listdir(wandb.config.path_trained_model)\n",
    "\t\t\tfile_list.sort()\n",
    "\t\t\tif len(file_list) > 0:\n",
    "\t\t\t\tlast_model = os.listdir(wandb.config.path_trained_model)[-1]\n",
    "\t\t\t# save_path = os.path.join(path, 'model_{}.pth'.format(begin_epoch))\n",
    "\t\t\tload_path = os.path.join(wandb.config.path_trained_model, last_model)\n",
    "\t\t\tbegin_epoch = load_model(load_path, optimizer, model)\n",
    "\t\tmodel.to(device)\n",
    "\n",
    "\t\toptimizer = torch.optim.__dict__[wandb.config.optim_type](params=model.parameters(), lr=wandb.config.learning_rate)\n",
    "\t\t\n",
    "\t\tmyloss = nn.MSELoss()\n",
    "\t\tscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True, min_lr=1e-6)\n",
    "\t\t#======================================================\n",
    "\t\tmodel.run_id = wandb.run.id\n",
    "\t\t#======================================================\n",
    "\t\t# config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer,y_normalizer,device\n",
    "\t\tfor epoch in range(begin_epoch, wandb.config.epochs+1):\n",
    "\t\t\tmodel, optimizer, train_loss_epoch = train_epoch(wandb.config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer,y_normalizer,device)\n",
    "\t\t\ttest_loss_epoch = eval_epoch(wandb.config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer_test, y_normalizer_test,device)\n",
    "\t\t\tnowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\t\t\tprint('epoch:', epoch, 'loss_train:', train_loss_epoch, 'loss_test:', test_loss_epoch)\n",
    "\t\t\t#======================================================\n",
    "\t\t\twandb.log({'epoch': epoch, 'loss_train': train_loss_epoch, 'loss_test': test_loss_epoch})\n",
    "\t\t\t#======================================================\n",
    "\t\t\tif epoch % 50 == 0:\n",
    "\t\t\t\tsave_path = os.path.join(wandb.config.path_trained_model, 'FNO_{}_{}.pth'.format(sweep_id,epoch))\n",
    "\t\t\t\tsave_model(save_path, epoch, optimizer, model)\n",
    "\t\twandb.finish()\n",
    "\treturn model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(is_model_saved=False):\n",
    "\t# 服务器里搜索时用的是: wandb.init(): 没有参数\n",
    "\ttrain_loader, test_loader, x_normalizer, y_normalizer, x_normalizer_test, y_normalizer_test = create_dataloader(config)\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\tactivation = torch.nn.__dict__[config.activation]()\n",
    "\tmodel = FNO2d(config.modes, config.modes, config.width, activation, config.dropout)\n",
    "\tmodel.apply(weight_init)\n",
    "\tbegin_epoch = 0\n",
    "\n",
    "\tif is_model_saved:\n",
    "\t\tfile_list = os.listdir(config.path_trained_model)\n",
    "\t\tfile_list.sort()\n",
    "\t\tif len(file_list) > 0:\n",
    "\t\t\tlast_model = os.listdir(config.path_trained_model)[-1]\n",
    "\t\t# save_path = os.path.join(path, 'model_{}.pth'.format(begin_epoch))\n",
    "\t\tload_path = os.path.join(config.path_trained_model, last_model)\n",
    "\t\tbegin_epoch = load_model(load_path, optimizer, model)\n",
    "\tmodel.to(device)\n",
    "\n",
    "\toptimizer = torch.optim.__dict__[config.optim_type](params=model.parameters(), lr=config.learning_rate)\n",
    "\t\n",
    "\tmyloss = nn.MSELoss()\n",
    "\tscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True, min_lr=1e-6)\n",
    "\t#======================================================\n",
    "\tnowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\twandb.init(project=config.project_name, config=config.__dict__, name=nowtime, save_code=True)\n",
    "\tmodel.run_id = wandb.run.id\n",
    "\t#======================================================\n",
    "\t# config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer,y_normalizer,device\n",
    "\tfor epoch in range(begin_epoch, config.epochs+1):\n",
    "\t\tmodel, optimizer, train_loss_epoch = train_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer,y_normalizer,device)\n",
    "\t\ttest_loss_epoch = eval_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,x_normalizer_test, y_normalizer_test,device)\n",
    "\t\tnowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\t\tprint('epoch:', epoch, 'loss_train:', train_loss_epoch, 'loss_test:', test_loss_epoch)\n",
    "\t\t#======================================================\n",
    "\t\twandb.log({'epoch': epoch, 'loss_train': train_loss_epoch, 'loss_test': test_loss_epoch})\n",
    "\t\t#======================================================\n",
    "\t\tif epoch % 50 == 0:\n",
    "\t\t\tsave_path = os.path.join(config.path_trained_model, 'FNO_{}_{}.pth'.format(sweep_id,epoch))\n",
    "\t\t\tsave_model(save_path, epoch, optimizer, model)\n",
    "\twandb.finish()\n",
    "\treturn model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = train(is_model_saved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动sweep agent\n",
    "wandb.agent(sweep_id, train, count=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "\tteam_name = '423team',\n",
    "\tproject_name = 'FNO',\n",
    "\texpertiment_name = 'FNO',\n",
    "\twandb_log_path = r\"../../wandb_results/\",\n",
    "\tworker_num = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SweepWorker():\n",
    "\tdef __init__(self,args,sweep_id):\n",
    "\t\tself.args = args\n",
    "\t\tself.sweep_id = sweep_id\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twandb.agent(self.sweep_id, function=train)\n",
    "\t\ttime.sleep(5)\n",
    "\t\tlp.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_program(worker_num,args,sweep_id):\n",
    "\tprogram = lp.Program('wandb_sweep')\n",
    "\twith program.group('sweep_worker'):\n",
    "\t\tfor i in range(worker_num):\n",
    "\t\t\tprogram.add_node(lp.CourierNode(SweepWorker,args,sweep_id))\n",
    "\treturn program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_wandb(args):\n",
    "\trun_dir = Path(args.wandb_log_path)/args.project_name/args.expertiment_name\n",
    "\tif not run_dir.exists():\n",
    "\t\trun_dir.mkdir(str(run_dir))\n",
    "\t\n",
    "\tos.environ[\"ANDB_ENTITY\"] = args.team_name\n",
    "\tos.environ[\"WANDB_PROJECT\"] = args.project_name\n",
    "\tos.environ[\"WANDB_DIR\"] = str(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sweep(args):\n",
    "\tset_wandb(args)\n",
    "\n",
    "\tsweep_id = wandb.sweep(sweep_config)\n",
    "\tprogram = make_program(args.worker_num,args,sweep_id)\n",
    "\tlp.launch(program, launch_type='local_mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sweep(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
