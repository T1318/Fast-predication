{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "print(sys.path)\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.PDE_Net import DeepONet_NS, weight_init\n",
    "from utils.DataGenerate_DON import Dataset_DON\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from argparse import Namespace\n",
    "from utils.utilities3 import *\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 0.5\n",
    "n_x = int(8/dx)\n",
    "n_y = int(3/dx)\n",
    "p_x = int(800/n_x+1)\n",
    "p_y = int(300/n_y+1)\n",
    "self_split = 2\n",
    "\n",
    "config = Namespace(\n",
    "\tproject_name='DON',\n",
    "\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "\n",
    "\tdx=0.5,\n",
    "\tn_x=n_x,\n",
    "\tn_y=n_y,\n",
    "\tp_x=p_x,\n",
    "\tp_y=p_y,\n",
    "\tself_split=2,\n",
    "\n",
    "\tepochs=10000,\n",
    "\tbatch_size=6000,\n",
    "\tlearning_rate=0.0007565,\n",
    "\tdropout=0.4,\n",
    "\tweight_decay=0.00042,\n",
    "\tmax_norm=2.864,\n",
    "\n",
    "\tbranch_input=204,\n",
    "\ttrunk_input=2,\n",
    "\thidden_size=512,\n",
    "\tbranch_layer=6,\n",
    "\ttrunk_layer=4,\n",
    "\t\n",
    "\toptim_type='AdamW',\n",
    "\n",
    "\tpath_trained_model=r'trained_model',\n",
    "\tpath_label=r'../train_data/label',\n",
    "\tpath_label_test=r'../test_data/label',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sweep_config = {\n",
    "\t'method': 'random',\n",
    "}\n",
    "metric = {\n",
    "\t'name': 'loss_train',\n",
    "\t'goal': 'minimize'\n",
    "}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_config['parameters'] = {}\n",
    "sweep_config['parameters'].update({\n",
    "\t'project_name': {'value': 'DON'},\n",
    "\t'epochs': {'value': 10000},\n",
    "\t'dx': {'value': 0.5},\n",
    "\t'n_x': {'value': 16},\n",
    "\t'n_y': {'value': 6},\n",
    "\t'p_x': {'value': 51},\n",
    "\t'p_y': {'value': 51},\n",
    "\t'self_split': {'value': 2},\n",
    "\n",
    "\t'branch_input': {'value': 204},\n",
    "\t'trunk_input': {'value': 2},\n",
    "\t'hidden_size': {'value': 512},\n",
    "\n",
    "\t'path_trained_model': {'value': r'trained_model'},\n",
    "\t'path_label': {'value': r'../train_data/label'},\n",
    "\t'path_label_test': {'value': r'../test_data/label'},\n",
    "})\n",
    "sweep_config['parameters'].update({\n",
    "\t'batch_size': {'values': [600,1500,3000]},\n",
    "\t'branch_layer': {'values': [4,5,6,7]},\n",
    "\t'trunk_layer': {'values': [4,5,6,7]},\n",
    "\t'optim_type': {'values': ['Adam','SGD','AdamW']},\n",
    "\t'learning_rate': {'distribution': 'log_uniform_values', 'min': 1e-5, 'max': 1e-1},\n",
    "\t'dropout': {'distribution': 'q_uniform', 'q': 0.2, 'min': 0, 'max': 0.6},\n",
    "\t'weight_decay': {'distribution': 'log_uniform_values', 'min': 1e-5, 'max': 1e-1},\n",
    "\t'max_norm': {'distribution': 'uniform', 'min': 1, 'max': 10},\n",
    "})\n",
    "\n",
    "# 初始化sweep controller\n",
    "sweep_id = wandb.sweep(sweep_config, project='DON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config):\n",
    "\ttrain_label = torch.empty(0, config.p_x, config.p_y, 1)\n",
    "\tfile_list = os.listdir(config.path_label)\n",
    "\tfor file in file_list:\n",
    "\t\tif file.endswith('.npy'):\n",
    "\t\t\t\tlabel = np.load(os.path.join(config.path_label, file))\n",
    "\t\t\t\ttrain_label = torch.cat((train_label, torch.Tensor(label)), 0)\n",
    "\n",
    "\ttrain_bc = torch.cat((train_label[:,:,0,0], train_label[:,-1,:,0], train_label[:,:,-1,0], train_label[:,0,:,0]), axis=1)\n",
    "\n",
    "\ttrain_label = train_label.reshape(train_label.shape[0],-1)\n",
    "\n",
    "\ttest_label = torch.empty(0, config.p_x, config.p_y, 1)\n",
    "\tfile_list = os.listdir(config.path_label_test)\n",
    "\tfor file in file_list:\n",
    "\t\tif file.endswith('.npy'):\n",
    "\t\t\t\tlabel = np.load(os.path.join(config.path_label_test, file))\n",
    "\t\t\t\ttest_label = torch.cat((test_label, torch.Tensor(label)), 0)\n",
    "\n",
    "\ttest_bc = torch.cat((test_label[:,:,0,0], test_label[:,-1,:,0], test_label[:,:,-1,0], test_label[:,0,:,0]), axis=1)\n",
    "\n",
    "\ttest_label = test_label.reshape(test_label.shape[0],-1)\n",
    "\n",
    "\tx_data,y_data,x_min,x_max = normalize(train_bc, train_label)\n",
    "\tx_test,y_test,x_min_test,x_max_test = normalize(test_bc, test_label)\n",
    "\n",
    "\ttrain_loader = DataLoader(torch.utils.data.TensorDataset(x_data, y_data, x_min, x_max),batch_size=config.batch_size,shuffle=True)\n",
    "\ttest_loader = DataLoader(torch.utils.data.TensorDataset(x_test, y_test, x_min_test, x_max_test),batch_size=config.batch_size,shuffle=False)\n",
    "\n",
    "\tcoordinate = np.meshgrid(np.linspace(0,config.dx,config.p_x), np.linspace(0,config.dx,config.p_y))\n",
    "\tcoordinate = np.stack((coordinate[0],coordinate[1]), axis=-1).reshape(-1,2)\n",
    "\n",
    "\tcoordinate = torch.Tensor(coordinate).float().to(device)\n",
    "\n",
    "\treturn train_loader, test_loader, coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,coordinate,device):\n",
    "\tmodel.train()\n",
    "\ttrain_loss_epoch = 0\n",
    "\tfor batch in train_loader:\n",
    "\t\tloss_train = 0\n",
    "\t\tdata_x,data_y,data_min,data_max = batch\n",
    "\n",
    "\t\tx = data_x.float().to(device)\n",
    "\t\ty = data_y.float().to(device)\n",
    "\t\t# batch_min = data_min.float().to(device)\n",
    "\t\t# batch_max = data_max.float().to(device)\n",
    "\n",
    "\t\tpred = model(x, coordinate)\n",
    "\n",
    "\t\t# pred = inverse_normalize(pred,batch_min,batch_max)\n",
    "\t\t# y =inverse_normalize(y,batch_min,batch_max)\n",
    "\n",
    "\t\tloss_train = myloss(pred.clone(), y.clone())\n",
    "\t\tregularization_loss = 0\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tregularization_loss += torch.norm(param, p=2)\n",
    "\t\tloss_train = loss_train + config.weight_decay * regularization_loss\n",
    "\t\ttrain_loss_epoch = train_loss_epoch + loss_train.item()\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss_train.backward(retain_graph=True)\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), config.max_norm)\n",
    "\t\toptimizer.step()\n",
    "\ttrain_loss_epoch = train_loss_epoch / len(train_loader)\n",
    "\t# print('epoch:', epoch, 'loss_train:', train_loss_epoch)\n",
    "\t# scheduler.step(train_loss_epoch)\n",
    "\treturn model, optimizer, train_loss_epoch\n",
    "\n",
    "def eval_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,coordinate,device):\n",
    "\tmodel.eval()\n",
    "\ttest_loss_epoch = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch_test in test_loader:\n",
    "\t\t\tloss_test = 0\n",
    "\t\t\ttest_x,test_y,test_min,test_max = batch_test\n",
    "\t\t\t\n",
    "\t\t\ttest_x = test_x.float().to(device)\t\t\t# [length,time_step,51,51,3]\n",
    "\t\t\ttest_y = test_y.float().to(device)\t\t\t# [length,time_step,51,51,3]\n",
    "\t\t\t# batch_min_test = test_min.float().to(device)\n",
    "\t\t\t# batch_max_test = test_max.float().to(device)\n",
    "\n",
    "\t\t\tpred_test = model(test_x, coordinate)\n",
    "\n",
    "\t\t\t# pred_test = inverse_normalize(pred_test,batch_min_test,batch_max_test)\n",
    "\t\t\t# test_y = inverse_normalize(test_y,batch_min_test,batch_max_test)\n",
    "\n",
    "\t\t\tloss_test = myloss(pred_test.clone(), test_y.clone())\n",
    "\n",
    "\t\t\ttest_loss_epoch = test_loss_epoch + loss_test.item()\n",
    "\t\ttest_loss_epoch = test_loss_epoch/len(test_loader)\n",
    "\t\t# print('epoch:', epoch, 'loss_test:', test_loss_epoch)\n",
    "\treturn test_loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(is_model_saved=False):\n",
    "\n",
    "\ttrain_loader, test_loader, coordinate = create_dataloader(config)\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\tmodel = DeepONet_NS(config.branch_input, config.trunk_input, config.branch_layer, config.trunk_layer, config.hidden_size, config.dropout)\n",
    "\tmodel.apply(weight_init)\n",
    "\tbegin_epoch = 0\n",
    "\n",
    "\tif is_model_saved:\n",
    "\t\tfile_list = os.listdir(config.path_trained_model)\n",
    "\t\tif len(file_list) > 0:\n",
    "\t\t\tlast_model = os.listdir(config.path_trained_model)[-1]\n",
    "\t\tload_path = os.path.join(config.path_trained_model, last_model)\n",
    "\t\tbegin_epoch = load_model(load_path, optimizer, model)\n",
    "\tmodel.to(device)\n",
    "\n",
    "\toptimizer = torch.optim.__dict__[config.optim_type](params=model.parameters(), lr=config.learning_rate)\n",
    "\tmyloss = nn.MSELoss()\n",
    "\tscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, verbose=True, min_lr=1e-6)\n",
    "\t#======================================================\n",
    "\tnowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\twandb.init(project=config.project_name, config=config.__dict__, name=nowtime, save_code=True)\n",
    "\tmodel.run_id = wandb.run.id\n",
    "\t#======================================================\n",
    "\tfor epoch in range(begin_epoch, config.epochs+1):\n",
    "\t\tmodel, optimizer, train_loss_epoch = train_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,coordinate,device)\n",
    "\t\ttest_loss_epoch = eval_epoch(config,model,optimizer,myloss,scheduler,train_loader,test_loader,coordinate,device)\n",
    "\t\tnowtime = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\t\tprint('epoch:', epoch, 'loss_train:', train_loss_epoch, 'loss_test:', test_loss_epoch)\n",
    "\t\t#======================================================\n",
    "\t\twandb.log({'epoch': epoch, 'loss_train': train_loss_epoch, 'loss_test': test_loss_epoch})\n",
    "\t\t#======================================================\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\tsave_path = os.path.join(wandb.config.path_trained_model, 'DON_{}.pth'.format(epoch))\n",
    "\t\t\tsave_model(save_path, epoch, optimizer, model)\n",
    "\n",
    "\twandb.finish()\n",
    "\treturn model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = train(is_model_saved=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
